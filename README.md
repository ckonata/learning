# learning
A running log of things I'm learning.

<details>
  <summary>Computer Science</summary>

### Problem Solving and learning
|Resource|Progress|
|---|---|
|[Article: Problem Solving Skills](https://ryanstutorials.net/problem-solving-skills/)|✅|
|[Article: Techniques for Efficiently Learning Programming Languages](https://www.flyingmachinestudios.com/programming/learn-programming-languages-efficiently/)|✅|
|[Video: How might we learn?](https://andymatuschak.org/hmwl/)|✅|

### General Computer Science
|Resource|Progress|
|---|---|
|[Effective Computation in Physics]()`5/10 chapters`|⬜|

### Maths must
	
|Resource|Progress|
|---|---|
|[Course: Stanford CS109 Probability for Computer Scientists](https://www.youtube.com/watch?v=2MuDZIAzBMY&list=PLoROMvodv4rOpr_A7B9SriE_iZmkanvUg)`0/29 lectures`|⬜|
|[Book: A First Course in Probability, by Sheldon Ross](https://chengzhaoxi.xyz/download/pdf/book/A-First-Course-in-Probability.pdf)`0/10 chapters`|⬜|
|[Course: Introduction to Probability](https://uni.dcdev.ro/y2s2/ps/Introduction%20to%20Probability%20by%20Joseph%20K.%20Blitzstein,%20Jessica%20Hwang%20(z-lib.org).pdf)`0/13 chapters`|⬜|
|[Course: An Introduction to Statistical Learning ](https://www.youtube.com/watch?v=LvySJGj-88U&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ)|
|---|---|
|[Course: Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/video_galleries/video-lectures/)`0/34 lectures`|⬜|


### Maths complementary
|[Course: An Introduction to Statistical Learning with Applications in Python](https://www.youtube.com/watch?v=LvySJGj-88U&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ)|⬜|
|[Datacamp: Foundations of Probability in Python](https://www.datacamp.com/courses/foundations-of-probability-in-python)|⬜|
|[Datacamp: Introduction to Statistics](https://www.datacamp.com/courses/introduction-to-statistics)|⬜|
|[Datacamp: Introduction to Statistics in Python](https://www.datacamp.com/courses/introduction-to-statistics-in-python)|⬜|
|[Datacamp: Hypothesis Testing in Python](https://www.datacamp.com/courses/hypothesis-testing-in-python)|⬜|
|[Datacamp: Statistical Thinking in Python (Part 1)](https://www.datacamp.com/courses/statistical-thinking-in-python-part-1)|⬜|
|[Datacamp: Statistical Thinking in Python (Part 2)](https://www.datacamp.com/courses/statistical-thinking-in-python-part-2)|⬜|
|[Datacamp: Experimental Design in Python](https://datacamp.com/courses/experimental-design-in-python)|⬜|
|[Datacamp: Practicing Statistics Interview Questions in Python](https://www.datacamp.com/courses/practicing-statistics-interview-questions-in-python)|⬜|
|[edX: Essential Statistics for Data Analysis using Excel](https://www.edx.org/course/essential-statistics-data-analysis-using-microsoft-dat222x-1)|⬜|
|[Udacity: Intro to Inferential Statistics](https://www.udacity.com/course/intro-to-inferential-statistics--ud201)|⬜|
|[MIT 18.06 Linear Algebra, Spring 2005](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8)|⬜|
|[Udacity: Eigenvectors and Eigenvalues](https://www.udacity.com/course/eigenvectors-and-eigenvalues--ud104)|⬜|
|[Udacity: Linear Algebra Refresher](https://www.udacity.com/course/linear-algebra-refresher-course--ud953)|⬜|
|[Youtube: Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)|⬜|

### Databases
|Resource|Progress|
|---|---|

</details>

<details>
  <summary>Programming Languages</summary>

### Python Programming

|Resource|Progress|
|---|---|
|[Book: Learning Scientific Programming with Python](https://scipython.com/books/book2/)`0/10 chapters`|⬜|
|[Book: From Python to Numpy](https://www.labri.fr/perso/nrougier/from-python-to-numpy/)`0/10 chapters`|⬜|

</details>

<details>
  <summary>Machine Learning and Data Science</summary>

### Data Science
|Resource|Progress|
|---|---|
|[Interactive: DataWars Practice Data Sciencewith Real Life Projects](https://www.datawars.io/#features)|⬜|

### Machine Learning System Design
|Resource|Progress|
|---|---|
|[Book: Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)|⬜|
|[Book: Designing Machine Learning Systems]()|⬜|
|[Neetcode: System Design for Beginners](https://neetcode.io/courses/system-design-for-beginners/0)|⬜|
|[Neetcode: System Design Interview](https://neetcode.io/courses/system-design-interview)|⬜|
|[Datacamp: Customer Analytics & A/B Testing in Python](https://www.datacamp.com/courses/customer-analytics-ab-testing-in-python)|⬜|
|[Datacamp: A/B Testing in Python](https://www.datacamp.com/courses/ab-testing-in-python)|⬜|
|[Udacity: A/B Testing](https://www.udacity.com/course/ab-testing--ud257)|⬜|
|[Datacamp: MLOps Concepts](https://www.datacamp.com/courses/mlops-concepts)|⬜|
|[Datacamp: Machine Learning Monitoring Concepts](https://www.datacamp.com/courses/machine-learning-monitoring-concepts)|⬜|

### Traditional Machine Learning

|Resource|Progress|
|---|---|
|[StatQuest: A Gentle Introduction to Machine Learning](https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=2)`0/104 lessons`|⬜|
|[Course: Introduction to Machine Learning](https://sebastianraschka.com/blog/2021/ml-course.html)`0/6 lessons`|⬜|
|[Book: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)|⬜|
|[Book: A Machine Learning Primer](https://www.confetti.ai/assets/ml-primer/ml_primer.pdf)|⬜|
|[Book: Grokking Machine Learning](https://www.manning.com/books/grokking-machine-learning)|⬜|
|[Book: The StatQuest Illustrated Guide To Machine Learning](https://www.amazon.com/StatQuest-Illustrated-Guide-Machine-Learning/dp/B0BLM4TLPY)|⬜|
|[Book: Machine Learning: A Probabilistic Perspective](https://probml.github.io/pml-book/book0.html)|⬜|
|[Book: Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)|⬜|
|[Book: Probabilistic Machine Learning: Advanced Topics](https://probml.github.io/pml-book/book2.html)|⬜|
|[Book: Information Theory, Inference and Learning Algorithms](https://www.amazon.com/Information-Theory-Inference-Learning-Algorithms/dp/0521642981)|⬜|
|[Datacamp: Ensemble Methods in Python](https://www.datacamp.com/courses/ensemble-methods-in-python)|⬜|
|[Datacamp: Extreme Gradient Boosting with XGBoost](https://www.datacamp.com/courses/extreme-gradient-boosting-with-xgboost)|⬜|
|[Datacamp: Clustering Methods with SciPy](https://www.datacamp.com/courses/clustering-methods-with-scipy)|⬜|
|[Datacamp: Unsupervised Learning in Python](https://www.datacamp.com/courses/unsupervised-learning-in-python)|⬜|
|[Udacity: Segmentation and Clustering](https://www.udacity.com/course/segmentation-and-clustering--ud981)|⬜|
|[edX: Implementing Predictive Analytics with Spark in Azure HDInsight](https://www.edx.org/course/implementing-predictive-analytics-spark-microsoft-dat202-3x-2)|⬜|
|[Datacamp: Supervised Learning with scikit-learn](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn)|⬜|
|[Datacamp: Machine Learning with Tree-Based Models in Python](https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-python)|⬜|
|[Datacamp: Linear Classifiers in Python](https://www.datacamp.com/courses/linear-classifiers-in-python)|⬜|
|[Datacamp: Model Validation in Python](https://www.datacamp.com/courses/model-validation-in-python)|⬜|
|[Datacamp: Hyperparameter Tuning in Python](https://www.datacamp.com/courses/hyperparameter-tuning-in-python)|⬜|
|[Datacamp: HR Analytics in Python: Predicting Employee Churn](https://www.datacamp.com/courses/hr-analytics-in-python-predicting-employee-churn)|⬜|
|[Datacamp: Predicting Customer Churn in Python](https://www.datacamp.com/courses/predicting-customer-churn-in-python)|⬜|
|[Datacamp: Dimensionality Reduction in Python](https://www.datacamp.com/courses/dimensionality-reduction-in-python)|⬜|
|[Datacamp: Preprocessing for Machine Learning in Python](https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python)|⬜|
|[Datacamp: Data Types for Data Science](https://www.datacamp.com/courses/data-types-for-data-science)|⬜|
|[Datacamp: Cleaning Data in Python](https://www.datacamp.com/courses/cleaning-data-in-python)|⬜|
|[Datacamp: Feature Engineering for Machine Learning in Python](https://www.datacamp.com/courses/feature-engineering-for-machine-learning-in-python)|⬜|
|[Datacamp: Predicting CTR with Machine Learning in Python](https://www.datacamp.com/courses/predicting-ctr-with-machine-learning-in-python)|⬜|
|[Datacamp: Intro to Financial Concepts using Python](https://www.datacamp.com/courses/intro-to-financial-concepts-using-python)|⬜|
|[Datacamp: Fraud Detection in Python](https://www.datacamp.com/courses/fraud-detection-in-python)|⬜|


### Deep Learning

|Resource|Progress|
|---|---|
|[Course: Deep Learning for Computer Vision](https://www.youtube.com/playlist?list=PLzUTmXVwsnXod6WNdg57Yc3zFx_f-RYsq)`0/16 lectures`|⬜|
|[Course: Deep Learning for Computer Vision - Notes](https://cs231n.github.io/classification/)`0/16 lectures`|⬜|
|[Course: Practical Deep Learning](https://course.fast.ai)`0/25 lectures`|⬜|
|[Course: Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html)`0/5 lessons`|⬜|
|[Course: Deep Learning Fundamentals](https://lightning.ai/courses/deep-learning-fundamentals/)`0/10 lessons`|⬜|
|[Book: Dive into Deep Learning](https://d2l.ai/chapter_installation/index.html)`0/23 chapters`|⬜|
|[Full Stack Deep Learning Bootcamp by Berkeley (https://fullstackdeeplearning.com/course/2022/)|⬜|
|[Article: An overview of gradient descent optimization algorithms](https://www.ruder.io/optimizing-gradient-descent)|⬜|
|[Book: Make Your Own Neural Network](https://www.amazon.com/Make-Your-Own-Neural-Network/dp/1530826608)|⬜|
|[Fast.ai: Practical Deep Learning for Coder (Part 1)](https://course.fast.ai/)|⬜|
|[Fast.ai: Practical Deep Learning for Coder (Part 2)](https://course.fast.ai/Lessons/part2.html) `9, 13,14,17,18(48:10),19`|⬜|
|[Datacamp: Convolutional Neural Networks for Image Processing](https://www.datacamp.com/courses/convolutional-neural-networks-for-image-processing)|⬜|
|[Karpathy: Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero/)|⬜|
|[Article: Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)|⬜|
|[Article: Things that confused me about cross-entropy](https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/)|⬜|
|[Course: Probabilistic Graphical Models Specialization by Coursera](https://www.coursera.org/specializations/probabilistic-graphical-models)|

### Natural Language Processing

|Resource|Progress|
|---|---|
|[Course: Natural Language Processing with Deep Learning by Stanford](https://www.youtube.com/playlist?list=PLU40WL8Ol94IJzQtileLTqGZuXtGlLMP_)`0/19 lessons`|⬜|
|[Book: Natural Language Processing with Transformers](https://transformersbook.com/)|⬜|
|[Stanford CS224U: Natural Language Understanding \| Spring 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20)|⬜|
|[Stanford CS224N: Stanford CS224N: NLP with Deep Learning \| Winter 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)|⬜|
|[CMU: Low-resource NLP Bootcamp 2020](https://www.youtube.com/playlist?list=PL8PYTP1V4I8A1CpCzURXAUa6H4HO7PF2c)|⬜|
|[CMU Multilingual NLP 2020](http://demo.clab.cs.cmu.edu/11737fa20/)|⬜|
|[Datacamp: Feature Engineering for NLP in Python](https://www.datacamp.com/courses/feature-engineering-for-nlp-in-python)|⬜|
|[Datacamp: Natural Language Processing Fundamentals in Python](https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python)|⬜|
|[Datacamp: Regular Expressions in Python](https://www.datacamp.com/courses/regular-expressions-in-python)|⬜|
|[Datacamp: RNN for Language Modeling](https://www.datacamp.com/courses/recurrent-neural-networks-for-language-modeling-in-python)|⬜|
|[Datacamp: Natural Language Generation in Python](https://www.datacamp.com/courses/natural-language-generation-in-python)|⬜|
|[Datacamp: Building Chatbots in Python](https://www.datacamp.com/courses/building-chatbots-in-python)|⬜|
|[Datacamp: Sentiment Analysis in Python](https://www.datacamp.com/courses/sentiment-analysis-in-python)|⬜|
|[Datacamp: Machine Translation in Python](https://www.datacamp.com/courses/machine-translation-in-python)|⬜|
|[Article: The Unreasonable Effectiveness of Collocations](https://opensourceconnections.com/blog/2019/05/16/unreasonable-effectiveness-of-collocations/)|⬜|
|[Article: FuzzyWuzzy: Fuzzy String Matching in Python](https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/#)|⬜|
|[Article: Transformers: Origins](https://mark-riedl.medium.com/transformers-origins-1db4bdfcb3d1)|⬜|

#### Research Papers
|Resource|Progress|
|---|---|
|[Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067)|⬜|
|[Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)|⬜|
|[Compression of Deep Learning Models for Text: A Survey](https://arxiv.org/abs/2008.05221)|⬜|
|[A Survey on Text Classification: From Shallow to Deep Learning](https://arxiv.org/pdf/2008.00364.pdf)|⬜|
|[A Survey of Transformers](https://arxiv.org/abs/2106.04554)|⬜|
|[AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing](https://arxiv.org/abs/2108.05542)|⬜|
|[Graph Neural Networks for Natural Language Processing: A Survey](https://arxiv.org/abs/2106.06090)|⬜|
|[A Survey of Data Augmentation Approaches for NLP](https://arxiv.org/abs/2105.03075)|⬜|
|[A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios](https://aclanthology.org/2021.naacl-main.201.pdf)|⬜|
|[Evaluation of Text Generation: A Survey](https://arxiv.org/pdf/2006.14799.pdf) |⬜|
|[A Survey of Transfer learning In NLP](https://arxiv.org/pdf/2007.04239.pdf)|⬜|
|[A Systematic Survey of Prompting Methods in NLP](https://arxiv.org/pdf/2107.13586.pdf)|⬜|

### Generative AI

#### LLM Theory

|Resource|Progress|
|---|---|
|[Book: Hands-On Large Language Models: Language Understanding and Generation](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961)|⬜|
|[Book: AI Engineering: Building Applications with Foundation Models](https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302)|⬜|
|[Book: Designing Large Language Model Applications](https://www.oreilly.com/library/view/designing-large-language/9781098150495/)|⬜|
|[Book: Large Language Models: A Deep Dive: Bridging Theory and Practice](https://www.amazon.com/Large-Language-Models-Bridging-Practice/dp/3031656466)|⬜|
|[Book: Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)|⬜|
|[Course: Introduction to Reinforcement Learning by DeepMind](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)`0/10 lectures`|⬜|
|[Book: A Little Bit of Reinforcement Learning from Human Feedback](https://rlhfbook.com/)|⬜|
|[Book: Deep reinforcement learning - Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)|⬜|
|[Stanford CS236: Deep Generative Models](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8)|⬜| 
|[Course: Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)|⬜|
|[Article: You could have designed state of the art Positional Encoding](https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding)|⬜|
|[Article: From Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs](https://huggingface.co/spaces/huggingface/number-tokenization-blog)|⬜|
|[Article: SolidGoldMagikarp (plus, prompt generation)](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)|⬜|
|[Article: Sampling for Text Generation](https://huyenchip.com/2024/01/16/sampling.html)|⬜|
|[Article: Scaling test-time compute - a Hugging Face Space by HuggingFaceH4](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)|⬜|
|[Article: DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs](https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1)|⬜|
|[Article: The Illustrated DeepSeek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)|⬜|
|[Article: A Visual Guide to Reasoning LLMs](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)|⬜|
|[Article: Mamba Explained](https://thegradient.pub/mamba-explained/)|⬜|
|[Article: A Visual Guide to Mamba and State Space Models](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)|⬜|
|[Article: Patterns and Messages - Part 1 - The Missing Subscript](https://mccormickml.com/2025/02/18/patterns-and-messages-part-1-wo-i/)|⬜|
|[Article: How text diffusion works](https://pierce.dev/notes/how-text-diffusion-works/)|⬜|
|[Article: The Big LLM Architecture Comparison](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison)|⬜|
|[DeepLearning.AI: Pretraining LLMs](https://www.deeplearning.ai/short-courses/pretraining-llms)|⬜|
|[DeepLearning.AI: Reinforcement Learning from Human Feedback](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback)|⬜|
|[Karpathy: Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g) `1hr`|⬜|
|[Karpathy: Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) `2hr13m`|⬜|
|[Karpathy: Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU) `4hr1m`|⬜|
|[Youtube: A Hackers' Guide to Language Models](https://www.youtube.com/watch?v=jkrNMKz9pWU) `1hr30m`|⬜|
|[Karpathy: Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI) `3h31m`|⬜|
|[Youtube: 5 Years of GPTs with Finbarr Timbers](https://www.youtube.com/watch?v=YA0pzBYAV2Q&list=PLKlhhkvvU8-YxMP9hjEYJTJDCaGszrJIh&index=8&t=43s) `55m`|⬜|
|[Youtube: Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts) `1h44m`|⬜|
|[Youtube: LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo) `1h10m`|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (7): Prompting and Complex Reasoning](https://www.youtube.com/watch?v=1Faf1cTe3T8&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=2)|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (6): Instruction Tuning](https://www.youtube.com/watch?v=iWcGS0gCL1E&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=3)|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (12): Domain Specific Modeling: Code and Math](https://www.youtube.com/watch?v=qHNUVpKO2dc&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=4)|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (15): Tool Use and LLM Agent Basics](https://www.youtube.com/watch?v=a3SjRsqV9ZA&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=16)|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (14): Ensembling and Mixture of Experts](https://www.youtube.com/watch?v=E4Rg4qTw4xw&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=15)|⬜|
|[Youtube: A little guide to building Large Language Models in 2024](https://www.youtube.com/watch?v=2-SPH9hIKT8) `1h15m`|⬜|
|[Youtube: How to approach post-training for AI applications](https://www.youtube.com/watch?v=grpc-Wyy-Zg) `22m`|⬜|
|[Youtube: Speculations on Test-Time Scaling (o1) `47m`](https://www.youtube.com/watch?v=6PEJ96k1kiw)|⬜|
|[Youtube: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://youtu.be/XMnxKGVnEUc) `1h19m`|⬜|
|[Youtube: How DeepSeek Changes the LLM Story](https://www.youtube.com/watch?v=0eMzc-WnBfQ)|⬜|
|[Youtube: MIT EI seminar, Hyung Won Chung from OpenAI. "Don't teach. Incentivize."](https://www.youtube.com/watch?v=kYWUEV_e2ss) `35m`|⬜|
|[Youtube: How I use LLMs](https://youtu.be/EWvNQjAaOHw) `2h7m`|⬜|
|[Youtube: Simple Diffusion Language Models](https://youtu.be/WjAUX23vgfg)|⬜|
|[Youtube: Introduction to Reasoning LLMs](https://www.youtube.com/watch?v=AZhUhGsgz4s) `1hr`|⬜|
|[Youtube: Zed Inferred: Diffusion Language Models](https://youtu.be/oot4O9wMohw?list=LL)|⬜|

#### Multi-modality

|Resource|Progress|
|---|---|
|[Article: Understanding Multimodal LLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms)|⬜|
|[Article: GPT-4 Vision Alternatives](https://blog.roboflow.com/gpt-4-vision-alternatives/)|⬜|
|[Article: Computer-Using Agent](https://openai.com/index/computer-using-agent/)|⬜|
|[Article: Flow Matching in 5 Minutes](https://nrehiew.github.io/blog/flow_matching/)|⬜|
|[Youtube: AI Visions Live \| Merve Noyan \| Open-source Multimodality](https://www.youtube.com/watch?v=_TlhKHTgWjY) `54m`|⬜|
|[DeepLearning.AI: How Diffusion Models Work](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/)|⬜|
|[DeepLearning.AI: Prompt Engineering for Vision Models](https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/)|⬜|
|[DeepLearning.AI: Building Multimodal Search and RAG](https://www.deeplearning.ai/short-courses/building-multimodal-search-and-rag/)|⬜|
|[Pinecone: Embedding Methods for Image Search](https://www.pinecone.io/learn/series/image-search/)|0/8|
|[Youtube: Lesson 9A 2022 - Stable Diffusion deep dive](https://youtu.be/0_BBRNYInx8)|⬜|
|[Article: Diffusion models are autoencoders](https://sander.ai/2022/01/31/diffusion.html)|⬜|
|[Article: Diffusion Language Models](https://sander.ai/2023/01/09/diffusion-language.html)|⬜|
|[Article: Guidance: a cheat code for diffusion models](https://sander.ai/2022/05/26/guidance.html)|⬜|
|[Article: Perspectives on diffusion](https://sander.ai/2023/07/20/perspectives.html)|⬜|
|[Article: The geometry of diffusion guidance](https://sander.ai/2023/08/28/geometry.html)|⬜|
|[Article: Diffusion is spectral autoregression](https://sander.ai/2024/09/02/spectral-autoregression.html)|⬜|
|[Article: Generative modelling in latent space](https://sander.ai/2025/04/15/latents.html)|⬜|
|[Article: Voice AI & Voice Agents - An Illustrated Primer](https://voiceaiandvoiceagents.com/)|⬜|
|[Youtube: Sander Dieleman - Generative modelling through iterative refinement](https://www.youtube.com/watch?v=9BHQvQlsVdE)|⬜|
|[Speech AI models: an introduction](https://thomwolf.io/blog/speech-ai.html)|⬜|


#### Information Retrieval / RAG

| Resource | Progress |
|---|---|
|[Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)|⬜|
| [Article: Pretrained Transformer Language Models for Search - part 1](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-1/#) |⬜|
| [Article: Pretrained Transformer Language Models for Search - part 2](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-2/)  |⬜|
| [Article: Pretrained Transformer Language Models for Search - part 3](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-3)   |⬜|
| [Article: Pretrained Transformer Language Models for Search - part 4](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-4)  |⬜|
|[Article: How not to use BERT for Document Ranking](https://bergum.medium.com/how-not-to-use-bert-for-search-ranking-4586716428d9)|⬜|
| [Article: Understanding LanceDB's IVF-PQ index](https://lancedb.github.io/lancedb/concepts/index_ivfpq/)|⬜|     
| [Article: A little pooling goes a long way for multi-vector representations](https://www.answer.ai/posts/colbert-pooling.html)|⬜|
|[Article: Levels of Complexity: RAG Applications](https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/)|⬜|
|[Article: Systematically Improving Your RAG](https://jxnl.github.io/blog/writing/2024/05/22/systematically-improving-your-rag/)|⬜|
|[Article: Stop using LGTM@Few as a metric (Better RAG)](https://jxnl.github.io/blog/writing/2024/02/05/when-to-lgtm-at-k/)|⬜|
|[Article: Low-Hanging Fruit for RAG Search](https://jxnl.github.io/blog/writing/2024/05/11/low-hanging-fruit-for-rag-search/)|⬜|
|[Article: What AI Engineers Should Know about Search](https://softwaredoug.com/blog/2024/06/25/what-ai-engineers-need-to-know-search)|⬜|
|[Article: Evaluating Chunking Strategies for Retrieval](https://research.trychroma.com/evaluating-chunking)|⬜|
|[Article: Sentence Embeddings. Introduction to Sentence Embeddings](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)|⬜|
|[Article: LambdaMART in Depth](https://softwaredoug.com/blog/2022/01/17/lambdamart-in-depth)|⬜|
|[Article: Guided Generation with Outlines](https://medium.com/canoe-intelligence-technology/guided-generation-with-outlines-c09a0c2ce9eb)|⬜|
|[Article: RAG tricks from the trenches](https://duarteocarmo.com/blog/rag-tricks-from-the-trenches)|⬜|
|[Article: Retrieval 101](https://isaacflath.com/blog/blog_post?fpath=posts%2F2025-03-17-Retrieval101.ipynb)|⬜|
|[Arxiv: Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)|⬜|
| [Course: Fullstack Retrieval](https://community.fullstackretrieval.com/)|⬜|
|[DeepLearning.AI: Building and Evaluating Advanced RAG Applications](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)|⬜|
|[DeepLearning.AI: Vector Databases: from Embeddings to Applications](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/)|⬜|
|[DeepLearning.AI: Advanced Retrieval for AI with Chroma](https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/)|⬜|
|[DeepLearning.AI: Prompt Compression and Query Optimization](https://www.deeplearning.ai/short-courses/prompt-compression-and-query-optimization/)|⬜|
|[DeepLearning.AI: Large Language Models with Semantic Search](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search) `1hr`|⬜|
|[DeepLearning.AI: Building Applications with Vector Databases](https://www.deeplearning.ai/short-courses/building-applications-vector-databases/)|⬜|
|[DeepLearning.AI: Knowledge Graphs for RAG](https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/)|⬜|
|[DeepLearning.AI: Preprocessing Unstructured Data for LLM Applications](https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications/)|⬜|
|[DeepLearning.AI: Embedding Models: From Architecture to Implementation](https://www.deeplearning.ai/short-courses/embedding-models-from-architecture-to-implementation)|⬜|
|[DeepLearning.AI: Retrieval Optimization - From Tokenization to Vector Quantization](https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization/)|⬜|
|[Pinecone: Vector Databases in Production for Busy Engineers](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/)|⬜|
|[Pinecone: Retrieval Augmented Generation](https://www.pinecone.io/learn/series/rag/)|⬜|
|[Pinecone: Faiss: The Missing Manual](https://www.pinecone.io/learn/series/faiss/)|⬜|
|[Pinecone: Natural Language Processing for Semantic Search](https://www.pinecone.io/learn/series/nlp/)|0/13|
|[Youtube: Systematically improving RAG applications](https://youtu.be/RrDBV6odPKo?list=PLgIaq8VgndJvXkDSeReTl2u4rQMShkZ6V)|⬜|
|[Youtube: Back to Basics for RAG w/ Jo Bergum](https://www.youtube.com/watch?v=nc0BupOkrhI&list=PLgIaq8VgndJvXkDSeReTl2u4rQMShkZ6V&index=2)|⬜|
|[Youtube: Beyond the Basics of Retrieval for Augmenting Generation (w/ Ben Clavié)](https://www.youtube.com/watch?v=0nA5QG3087g&t=1287s)|⬜|
|[Youtube: RAG From Scratch](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x) `14/14`|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (10): Retrieval and RAG](https://www.youtube.com/watch?v=KfQaYk4k9eM&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=6) `1h17m`|⬜|
|[Guidance: Token Healing](https://github.com/guidance-ai/guidance/blob/main/notebooks/tutorials/token_healing.ipynb)|⬜|
|[Youtube: What You See Is What You Search: Vision Language Models for PDF Retrieval [Jo Bergum]](https://youtu.be/qrbQUU4TrLM)|⬜|

#### Agentic Engineering

|Resource|Progress|
|---|---|
|[Berkeley: CS294/194-196 Large Language Model Agents](https://www.youtube.com/playlist?list=PLS01nW3RtgopsNLeM936V4TNSsvvVglLc) `0/14 lectures`|⬜|
|[Berkeley: Advanced LLM Agents MOOC](https://www.youtube.com/playlist?list=PLS01nW3RtgorL3AW8REU9nGkzhvtn6Egn) `0/12 lectures`|⬜|
|[Article: Tool Invocation - Demonstrating the Marvel of GPT's Flexibility](https://blog.jnbrymn.com/2024/01/30/the-marvel-of-GPT-generality.html)|⬜|
|[Article: Introducing smolagents, a simple library to build agents](https://huggingface.co/blog/smolagents)|⬜|
|[Article: What Problem Does The Model Context Protocol Solve?](https://www.aihero.dev/what-problem-does-model-context-protocol-solve)|⬜|
|[Article: Don’t Build Multi-Agents](https://cognition.ai/blog/dont-build-multi-agents)|⬜|
|[Article: Coding Agents 101: The Art of Actually Getting Things Done](https://devin.ai/agents101)|⬜|
|[Anthropic: Building effective agents](https://www.anthropic.com/research/building-effective-agents)|⬜|
|[Anthropic: Building Effective Agents Cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents)|⬜|
|[OpenAI: Assistants & Agents Build Hour](https://vimeo.com/showcase/11333741/video/990334325)|⬜|
|[OpenAI: Function Calling Build Hour](https://vimeo.com/showcase/11333741/video/952127114)|⬜|
|[DeepLearning.AI: Functions, Tools and Agents with LangChain](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/)|⬜|
|[DeepLearning.AI: Building Agentic RAG with LlamaIndex](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/)|⬜|
|[DeepLearning.AI: Multi AI Agent Systems with crewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/)|⬜|
|[DeepLearning.AI: Building Towards Computer Use with Anthropic](https://www.deeplearning.ai/short-courses/building-towards-computer-use-with-anthropic/)|⬜|
|[DeepLearning.AI: Practical Multi AI Agents and Advanced Use Cases with crewAI](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/)|⬜|
|[DeepLearning.AI: LLMs as Operating Systems: Agent Memory](https://www.deeplearning.ai/short-courses/llms-as-operating-systems-agent-memory/)|⬜|
|[DeepLearning.AI: Serverless Agentic Workflows with Amazon Bedrock](https://www.deeplearning.ai/short-courses/serverless-agentic-workflows-with-amazon-bedrock/)|⬜|
|[DeepLearning.AI: AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/)|⬜|
|[DeepLearning.AI: AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)|⬜|
|[DeepLearning.AI: Building Your Own Database Agent](https://www.deeplearning.ai/short-courses/building-your-own-database-agent/)|⬜|
|[DeepLearning.AI: Function-Calling and Data Extraction with LLMs](https://www.deeplearning.ai/short-courses/function-calling-and-data-extraction-with-llms/) `59m`|⬜|
|[DeepLearning.AI: Evaluating AI Agents](https://www.deeplearning.ai/short-courses/evaluating-ai-agents/) `2h16m`|⬜|
|[DeepLearning.AI: Build Apps with Windsurf’s AI Coding Agents](https://www.deeplearning.ai/short-courses/build-apps-with-windsurfs-ai-coding-agents/) `1h10m`|⬜|
|[DeepLearning.AI: Building AI Browser Agents](https://www.deeplearning.ai/short-courses/building-ai-browser-agents)|⬜|
|[Huggingface: Agents Course](https://huggingface.co/learn/agents-course/unit1/messages-and-special-tokens#base-models-vs-instruct-models)|Unit 1|
|[Youtube: How to Evaluate Agents: Galileo’s Agentic Evaluations in Action](https://www.youtube.com/watch?v=QvStk5G8BZw)|⬜|
|[Youtube: Agent Response \| LangSmith Evaluation - Part 24](https://youtu.be/NbQKDfSw3gM?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S)|⬜|
|[Youtube: Single Step \| LangSmith Evaluation - Part 25](https://youtu.be/AVPflFmRkd4?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S)|⬜|
|[Youtube: Agent Trajectory \| LangSmith Evaluation - Part 26](https://youtu.be/pvlT056DAHs?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S)|⬜|
|[Youtube: Evaluating Agents and Assistants: The AI Conference](https://www.youtube.com/watch?v=6uXWhmDRcMc)|⬜|
|[Youtube: How to Build, Evaluate, and Iterate on LLM Agents](https://youtu.be/0pnEUAwoDP0)|⬜|
|[Youtube: Mem0: Building AI Agents with Scalable Long-Term Memory](https://www.youtube.com/watch?v=EE4pvOEAjXc)|⬜|


#### Context Engineering

|Resource|Progress|
|---|---|
|[Article: OpenAI Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)|⬜|
|[Article: Prompting Fundamentals and How to Apply them Effectively](https://eugeneyan.com/writing/prompting/)|⬜|
|[Article: How I came in first on ARC-AGI-Pub using Sonnet 3.5 with Evolutionary Test-time Compute](https://params.com/@jeremy-berman/arc-agi)|⬜|
|[Anthropic Courses](https://github.com/anthropics/courses)|⬜|
|[Anthropic: The Claude in Amazon Bedrock Course](https://www.anthropic.com/aws-reinvent-2024/course)|⬜|
|[Article: Prompt Engineering(Liliang Weng)](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)|⬜|
|[Article: Prompt Engineering 201: Advanced methods and toolkits](https://amatria.in/blog/prompt201)|⬜|
|[Article: Optimizing LLMs for accuracy](https://platform.openai.com/docs/guides/optimizing-llm-accuracy)|⬜|
|[Article: Primers • Prompt Engineering](https://aman.ai/primers/ai/prompt-engineering/)|⬜|
|[Article: Anyscale Endpoints: JSON Mode and Function calling Features](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features)|⬜|
|[Article: Guided text generation with Large Language Models](https://medium.com/productizing-language-models/guided-text-generation-with-large-language-models-d88fc3dcf4c)|⬜|
|[Anthropic: AI Fluency](https://www.anthropic.com/ai-fluency)|⬜|
|[Book: Prompt Engineering for LLMs](https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/)|⬜|
|[DeepLearning.AI: Reasoning with o1](https://www.deeplearning.ai/short-courses/reasoning-with-o1/)|⬜|
|[OpenAI: Reasoning with o1 Build Hour](https://vimeo.com/showcase/11333741/video/1018737829)|⬜|
|[DeepLearning.AI: ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)|⬜|
|[DeepLearning.AI: Prompt Engineering with Llama 2 & 3](https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/)|⬜|
|[Wandb: LLM Engineering: Structured Outputs](https://www.wandb.courses/courses/steering-language-models)|⬜|
|[Series: Prompt injection](https://simonwillison.net/series/prompt-injection/)|⬜|
|[Youtube: Prompt Engineering Overview](https://www.youtube.com/watch?v=dOxUroR57xs) |⬜|
|[Youtube: Prompt Engineering Workshop](https://youtu.be/htBTho6oEJA) |⬜|

#### Quantization
|Resource|Progress|
|---|---|
|[Article: Quantization Fundamentals with Hugging Face](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)|⬜|
|[DeepLearning.AI: Quantization in Depth](https://www.deeplearning.ai/short-courses/quantization-in-depth/)|⬜|
|[DeepLearning.AI: Introduction to On-Device AI](https://www.deeplearning.ai/short-courses/introduction-to-on-device-ai/)|⬜|
|[Article: A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)|⬜|
|[Article: QLoRA and 4-bit Quantization](https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/)|⬜|
|[Article: Understanding AI/LLM Quantisation Through Interactive Visualisations](https://smcleod.net/2024/07/understanding-ai/llm-quantisation-through-interactive-visualisations/)|⬜|
|[Youtube: CMU Advanced NLP Fall 2024 (11): Distillation, Quantization, and Pruning](https://www.youtube.com/watch?v=DvVGkj4zhVU&list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp&index=5)|⬜|
|[Article: LLM.int8() and Emergent Features](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)|⬜|

#### Distributed Training

|Resource|Progress|
|---|---|
|[PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs](https://sebastianraschka.com/teaching/pytorch-1h/) `0/9 lessons`|⬜|
|[Youtube: Slaying OOMs with PyTorch FSDP and torchao](https://youtu.be/UvRl4ansfCg)|⬜|
|[Youtube: Distributed Training with PyTorch: complete tutorial with cloud infrastructure and code](https://youtu.be/toUSzwR0EV8)|⬜|
|[Youtube: How DDP works \|\| Distributed Data Parallel ](https://youtu.be/bwNtfxEDjGA)|⬜|
|[Youtube: FSDP Explained](https://youtu.be/6pVn6khIgiI)|⬜|
|[Youtube: Lecture 48: The Ultra Scale Playbook](https://youtu.be/1E8GDR8QXKw) |⬜|
|[Youtube: Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li](https://youtu.be/3XUG7cjte2U)|⬜|
|[Youtube: Unit 9 \| Techniques for Speeding Up Model Training](https://www.youtube.com/playlist?list=PLaMu-SDt_RB403GN5DU7NYVoVmO5Vsgkh)|⬜|
|[Article: A Short Guide to PyTorch DDP](https://blog.hpc.qmul.ac.uk/pytorch-ddp/)|⬜|
|[Article: Scaling Deep Learning with PyTorch: Multi-Node and Multi-GPU Training Explained (with Code)](https://medium.com/@ashraf.kasem.94.0/scaling-deep-learning-with-pytorch-multi-node-and-multi-gpu-training-explained-with-code-ece8f03ea59b)|⬜|
|[Article: Accelerating PyTorch Model Training](https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training)|⬜|
|[Article: Meet Horovod: Uber’s Open Source Distributed Deep Learning Framework for TensorFlow](https://www.uber.com/blog/horovod/)|⬜|
|[Article: Distributed data parallel training in Pytorch](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)|⬜|
|[Article: Training on Multiple GPUs](https://d2l.ai/chapter_computational-performance/multiple-gpus.html)|⬜|


#### Parallel Computing

|Resource|Progress|
|---|---|
|[Udacity: Intro to Parallel Programming](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2) |⬜|
|[Book: Programming Massively Parallel Processors: A Hands-on Approach](https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0124159923)|Ch. 2|
|[Youtube: GPU Puzzles: Let's Play](https://youtu.be/K4T-YwsOxrM)|⬜|

#### Inference Optimization

|Resource|Progress|
|---|---|
|[Article: How to make LLMs go fast](https://vgel.me/posts/faster-inference/)|⬜|
|[Article: In the Fast Lane! Speculative Decoding - 10x Larger Model, No Extra Cost](https://docs.titanml.co/blog/speculative-decoding-unleashed/)|⬜|
|[Article: Accelerating Generative AI with PyTorch II: GPT, Fast](https://pytorch.org/blog/accelerating-generative-ai-2/)|⬜|
|[Article: Harmonizing Multi-GPUs: Efficient Scaling of LLM Inference](https://docs.titanml.co/blog/multi-gpu/)|⬜|
|[Article: Multi-Query Attention is All You Need](https://fireworks.ai/blog/multi-query-attention-is-all-you-need)|⬜|
|[Article: Transformers Inference Optimization Toolset](https://astralord.github.io/posts/transformer-inference-optimization-toolset/)|⬜|
|[DeepLearning.AI: Efficiently Serving LLMs](https://www.deeplearning.ai/short-courses/efficiently-serving-llms/)|⬜|
|[Article: LLM Inference Series: 3. KV caching explained](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8)|⬜|
|[Article: LLM Inference Series: 4. KV caching, a deeper look](https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8)|⬜|
|[Article: LLM Inference Series: 5. Dissecting model performance](https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f)|⬜|
|[Article: Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/)|⬜|
|[Article: Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/)|⬜|
|[Article: Optimizing AI Inference at Character.AI (Part Deux)](https://research.character.ai/optimizing-ai-inference-at-character-ai-part-deux/)|⬜|
|[Article: llama.cpp guide - Running LLMs locally, on any hardware, from scratch](https://blog.steelph0enix.dev/posts/llama-cpp-guide/)|⬜|
|[Article: Domain specific architectures for AI inference](https://fleetwood.dev/posts/domain-specific-architectures)|⬜|
|[Youtube: SBTB 2023: Charles Frye, Parallel Processors: Past & Future Connections Between LLMs and OS Kernels](https://www.youtube.com/watch?v=VxFtHqlMv8c)|⬜|
|[Youtube: Deploying Fine-Tuned Models](https://youtu.be/GzEcyBykkdo)|⬜|
|[Article: Compiling ML models to C for fun](https://bernsteinbear.com/blog/compiling-ml-models/)|⬜|
|[Article: How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)|⬜|

#### Evals and Guardrails

|Resource|Progress|
|---|---|
|[Article: Your AI Product Needs Evals](https://hamel.dev/blog/posts/evals)|⬜|
|[Article: Task-Specific LLM Evals that Do & Don't Work](https://eugeneyan.com/writing/evals/)|⬜|
|[Article: Evaluation & Hallucination Detection for Abstractive Summaries](https://eugeneyan.com/writing/abstractive/)|⬜|
|[Article: Aligning LLM as judge with human evaluators](https://blog.ragas.io/aligning-llm-as-judge-with-human-evaluators)|⬜|
|[Article: Hard-Earned Lessons from 2 Years of Improving AI Applications](https://blog.ragas.io/hard-earned-lessons-from-2-years-of-improving-ai-applications)|⬜|
|[Article: Evaluating Long-Context Question & Answer Systems](https://eugeneyan.com/writing/qa-evals/)|⬜|
|[DeepLearning.AI: Automated Testing for LLMOps](https://www.deeplearning.ai/short-courses/automated-testing-llmops/)|⬜|
|[DeepLearning.AI: Red Teaming LLM Applications](https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/)|⬜|
|[DeepLearning.AI: Evaluating and Debugging Generative AI Models Using Weights and Biases](https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai/)|⬜|
|[DeepLearning.AI: Quality and Safety for LLM Applications](https://www.deeplearning.ai/short-courses/quality-safety-llm-applications/)|⬜|
|[OpenAI: Evals Build Hour](https://vimeo.com/showcase/11333741/video/1023317525)|⬜|
|[Youtube: Instrumenting & Evaluating LLMs](https://youtu.be/SnbGD677_u0) |⬜|
|[Youtube: LLM Eval For Text2SQL](https://youtu.be/UGmenkjGXqM?list=PLgIaq8VgndJvt-HKMHPXehyJNNXQsAVHD) |⬜|
|[Youtube: A Deep Dive on LLM Evaluation](https://youtu.be/IsZVCnViwhk?list=PLgIaq8VgndJvt-HKMHPXehyJNNXQsAVHD) |⬜|

### Finetuning and Distillation

|Resource|Progress|
|---|---|
|[Article: Tokenization Gotchas](https://hamel.dev/notes/llm/finetuning/tokenizer_gotchas.html)|⬜|
|[Article: Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)|⬜|
|[OpenAI: GPT-4o mini Fine-Tuning Build Hour](https://vimeo.com/showcase/11333741/video/995989828)|⬜|
|[OpenAI: Distillation Build Hour](https://vimeo.com/showcase/11333741/video/1029408095)|⬜|
|[Article: How to Generate and Use Synthetic Data for Finetuning](https://eugeneyan.com/writing/synthetic/)|⬜|
|[DeepLearning.AI: Finetuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)|⬜|
|[Youtube: Fine-Tuning with Axolotl](https://youtu.be/mmsa4wDsiy0?list=PLgIaq8VgndJtZ_G6gxyuhHGLUy9zXV9JC) |⬜|
|[Youtube: Creating, Curating, and Cleaning Data for LLMs](https://youtu.be/HEGaei7k0zE?list=PLgIaq8VgndJtZ_G6gxyuhHGLUy9zXV9JC) |⬜|
|[Youtube: Best Practices For Fine Tuning Mistral](https://youtu.be/Z_oWzTuljss?list=PLgIaq8VgndJtZ_G6gxyuhHGLUy9zXV9JC) |⬜|
|[Youtube: Fine Tuning OpenAI Models - Best Practices](https://youtu.be/Q0GSZD0Na1s?list=PLgIaq8VgndJtZ_G6gxyuhHGLUy9zXV9JC)|⬜|
|[Youtube: When and Why to Fine Tune an LLM](https://youtu.be/cPn0nHFsvFg) |⬜|
|[Youtube: Napkin Math For Fine Tuning Pt. 1 w/Johno Whitaker](https://youtu.be/-2ebSQROew4)|⬜|
|[Youtube: Napkin Math For Fine Tuning Pt. 2 w/Johno Whitaker](https://youtu.be/u2fJ6K8FjS8)|⬜|
|[Youtube: Fine Tuning LLMs for Function Calling w/Pawel Garback](https://youtu.be/SEZ7j31u67A) |⬜|
|[Youtube: From Prompt to Model: Fine-tuning when you've already deployed LLMs in prod w/Kyle Corbitt](https://youtu.be/4EPZZkVrXC4) |⬜|
|[Youtube: Why Fine Tuning is Dead w/Emmanuel Ameisen](https://youtu.be/h1c_jmk97Ss) |⬜|
|[Benchmarking QLoRA+FSDP](https://github.com/AnswerDotAI/fsdp_qlora/blob/main/benchmarks_03_2024.md)|⬜|

#### LLM System Design

|Resource|Progress|
|---|---|
|[Article: What We’ve Learned From A Year of Building with LLMs](https://applied-llms.org/)|⬜|
|[Article: Data Flywheels for LLM Applications](https://www.sh-reya.com/blog/ai-engineering-flywheel/)|⬜|
|[Article: LLM From the Trenches: 10 Lessons Learned Operationalizing Models at GoDaddy](https://www.godaddy.com/resources/news/llm-from-the-trenches-10-lessons-learned-operationalizing-models-at-godaddy#h-3-prompts-aren-t-portable-across-models)|⬜|
|[Article: Emerging UX Patterns for Generative AI Apps & Copilots](https://www.tidepool.so/blog/emerging-ux-patterns-for-generative-ai-apps-copilots)|⬜|
|[Article: The Novice's LLM Training Guide](https://rentry.co/llm-training)|⬜|
|[Article: Pushing ChatGPT's Structured Data Support To Its Limits](https://minimaxir.com/2023/12/chatgpt-structured-data/)|⬜|
|[Article: GPTed: using GPT-3 for semantic prose-checking](https://vgel.me/posts/gpted-launch/)|⬜|
|[Article: Don't worry about LLMs](https://vickiboykis.com/2024/05/20/dont-worry-about-llms/)|⬜|
|[Article: Things we learned about LLMs in 2024](https://simonwillison.net/2024/Dec/31/llms-in-2024/)|⬜|
|[Article: Data acquisition strategies for AI-first start-ups](https://press.airstreet.com/p/data-acquisition-strategies-for-ai?utm_source=substack&utm_medium=email)|⬜|
|[Article: All about synthetic data generation](https://blog.ragas.io/all-about-synthetic-data-generation)|⬜|
|[DeepLearning.AI: Building Systems with the ChatGPT API](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)|⬜|
|[DeepLearning.AI: Building Generative AI Applications with Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/)|⬜|
|[DeepLearning.AI: Open Source Models with Hugging Face](https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/)|⬜|
|[DeepLearning.AI: Getting Started with Mistral](https://www.deeplearning.ai/short-courses/getting-started-with-mistral/)|⬜|
|[LLMOps: Building with LLMs](https://www.comet.com/site/llm-course/)|⬜|
|[LLM Bootcamp - Spring 2023](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)|⬜|
|[Youtube: A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y)|⬜|
|[Youtube: Building Blocks for LLM Systems & Products: Eugene Yan](https://www.youtube.com/watch?v=LzeC1AQ-U5o)|⬜|
|[Youtube: Building LLM Applications](https://www.youtube.com/playlist?list=PLgIaq8VgndJtrxcelEdnXbvh9fXMHeAps)|0/8|
|[Article: Emerging Architectures for LLM Applications](https://a16z.com/emerging-architectures-for-llm-applications/)|⬜|
|[Article: Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)|⬜|
|[DeepLearning.AI: LLMOps](https://www.deeplearning.ai/short-courses/llmops/)|⬜|
|[DeepLearning.AI: Serverless LLM apps with Amazon Bedrock](https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock/)|⬜|
|[Youtube: Getting the Most Out of Your LLM Experiments](https://youtu.be/IfcDvtl6Z1Y) `48m`|⬜|

</details>

<details>
  <summary>Tooling</summary>
	
### VIM

|Resource|Progress|
|---|---|
|[Videos: Vim Novice Videos](http://derekwyatt.org/vim/tutorials/novice/)`0/9`|⬜|
|[Videos: Vim Intermediate Videos](http://derekwyatt.org/vim/tutorials/intermediate/)`0/7`|⬜|
|[Videos: Vim Advanced Videos](http://derekwyatt.org/vim/tutorials/advanced/)`0/4`|⬜|
|[Article: Why, oh WHY, do those #?@! nutheads use vi?](http://www.viemu.com/a-why-vi-vim.html)|⬜|

### PKM Tana
|Resource|Progress|
|---|---|
|[Youtube:How to Setup Tana in Minutes (Tana Beginner's Guide)](https://www.youtube.com/watch?v=pms07e4GEDo) |⬜|
|[Youtube:Learn Tana - Full Course for Beginners Tutorial](https://www.youtube.com/watch?v=Vlr2fNHqJWM) |⬜|
|[Youtube:Tana Tour with Andrea Grimsdatter Stallvik: Simple Student Workflows](https://www.youtube.com/watch?v=t-YpMobjMTI) |⬜|
|[Youtube:A Tour of Mark's Tana Setup]([https://www.youtube.com/watch?v=pms07e4GEDo](https://www.youtube.com/watch?v=K_m7YVqIgXo)) |⬜|
|[Youtube:100 TANA TIPS: Full 46-Minute Tana Beginner Course](https://www.youtube.com/watch?v=bdqf2u0iRS8)|⬜|
|[Youtube:Tanacast 01: Agents, Voice, Automation, AI-powered Daily Nodes](https://www.youtube.com/watch?v=P3n93hhEOkE)|⬜|
|[Youtube:Tanacast 02. Building AGENT in Tana from scratch](https://www.youtube.com/watch?v=rjAqcva5-2A)|⬜|
|[Article:My PKM System in Tana](https://medium.com/@bri-ballard/my-pkm-system-in-tana-4a8a8551bf02)|✅|
|[Article:How Tana is helping me rethink my Futures research workflow.](https://medium.com/foresight-toolstack/how-tana-is-helping-me-rethink-my-futures-research-workflow-3595ed9857a7)|✅|
|[Youtube:How CTOs Use Tana: 5 Genius Workflows](https://www.youtube.com/watch?v=Htp7OBBZc54)|✅|

### Tutti Frutti
|Resource|Progress|
|---|---|
|[Videos: The best tech talks for developers](https://dev.tube)|⬜|
|[Article: Agile software development articles(https://gojko.net/lists/agile.html)]|⬜|

</details>



<details>
  <summary>Waiting for category classification</summary>
- https://deep-learning-drizzle.github.io
- https://www.cs.ox.ac.uk/teaching/courses/
- https://huyenchip.com/mlops/
- https://huyenchip.com/2018/03/30/guide-to-Artificial-Intelligence-Stanford.html
- https://libroslibertarios.com.ar/economia/inicial/?mpage=3
 
</details>



